{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"../input\"))\n\n# Any results you write to the current directory are saved as output.","execution_count":1,"outputs":[{"output_type":"stream","text":"['final_merged.tsv']\n","name":"stdout"}]},{"metadata":{"_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","trusted":true},"cell_type":"code","source":"data = pd.read_csv('../input/final_merged.tsv', sep='\\t')","execution_count":2,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Read the final feature processed dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head()","execution_count":3,"outputs":[{"output_type":"execute_result","execution_count":3,"data":{"text/plain":"   User_Id  Prod_Id    ...     number_digit_Words noun_count\n0      923        0    ...                      0          3\n1      923       19    ...                      0          9\n2      923       40    ...                      1          4\n3      923       63    ...                      0          2\n4      923       79    ...                      0          3\n\n[5 rows x 19 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>User_Id</th>\n      <th>Prod_Id</th>\n      <th>Date_x</th>\n      <th>Review</th>\n      <th>Rating</th>\n      <th>Label</th>\n      <th>Product_Name</th>\n      <th>Avg_Prod_Rating</th>\n      <th>Avg_user_rating</th>\n      <th>Review_Len</th>\n      <th>user_total_reviews</th>\n      <th>corpus</th>\n      <th>compound</th>\n      <th>neg</th>\n      <th>neu</th>\n      <th>pos</th>\n      <th>number_Cap_Words</th>\n      <th>number_digit_Words</th>\n      <th>noun_count</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>923</td>\n      <td>0</td>\n      <td>08-12-2014</td>\n      <td>The food at snack is a selection of popular Gr...</td>\n      <td>3</td>\n      <td>-1</td>\n      <td>Snack</td>\n      <td>4.009524</td>\n      <td>4.435897</td>\n      <td>215</td>\n      <td>39</td>\n      <td>food snack select popular greek dish appet tra...</td>\n      <td>0.6486</td>\n      <td>0.061</td>\n      <td>0.693</td>\n      <td>0.247</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>923</td>\n      <td>19</td>\n      <td>14-01-2014</td>\n      <td>The restaurant is on the ground floor of a typ...</td>\n      <td>5</td>\n      <td>-1</td>\n      <td>Palo Santo</td>\n      <td>4.037152</td>\n      <td>4.435897</td>\n      <td>513</td>\n      <td>39</td>\n      <td>littl place soho wonder lamb sandwich glass wi...</td>\n      <td>-0.1280</td>\n      <td>0.189</td>\n      <td>0.673</td>\n      <td>0.138</td>\n      <td>0</td>\n      <td>0</td>\n      <td>9</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>923</td>\n      <td>40</td>\n      <td>30-05-2014</td>\n      <td>Really nice mousaka and lovely d√©cor inside. A...</td>\n      <td>4</td>\n      <td>-1</td>\n      <td>Pylos</td>\n      <td>4.312869</td>\n      <td>4.435897</td>\n      <td>231</td>\n      <td>39</td>\n      <td>order lunch snack last friday time noth miss f...</td>\n      <td>0.7717</td>\n      <td>0.067</td>\n      <td>0.628</td>\n      <td>0.305</td>\n      <td>0</td>\n      <td>1</td>\n      <td>4</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>923</td>\n      <td>63</td>\n      <td>13-11-2014</td>\n      <td>I really enjoyed brunch at Jane. The ambiance ...</td>\n      <td>4</td>\n      <td>-1</td>\n      <td>Jane</td>\n      <td>3.937181</td>\n      <td>4.435897</td>\n      <td>169</td>\n      <td>39</td>\n      <td>beauti quaint littl restaur pretti street stro...</td>\n      <td>0.8910</td>\n      <td>0.042</td>\n      <td>0.712</td>\n      <td>0.246</td>\n      <td>0</td>\n      <td>0</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>923</td>\n      <td>79</td>\n      <td>30-03-2014</td>\n      <td>We ate at the Blue Ribbon with colleagues. The...</td>\n      <td>3</td>\n      <td>-1</td>\n      <td>Blue Ribbon Brasserie</td>\n      <td>4.280000</td>\n      <td>4.435897</td>\n      <td>255</td>\n      <td>39</td>\n      <td>snack great place casual sit lunch especi cold...</td>\n      <td>0.9769</td>\n      <td>0.000</td>\n      <td>0.661</td>\n      <td>0.339</td>\n      <td>0</td>\n      <td>0</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"**Identifying the most common words for Fake and True reviews**"},{"metadata":{"trusted":true},"cell_type":"code","source":"positive = data[data['Label']== 1]\ntrue_word_list = []\nfor i in range(0,positive['corpus'].count()):\n   true_word_temp = positive['corpus'].iloc[i].split()\n   true_word_list.append(true_word_temp)\nfrom itertools import chain\nlist1 = list(chain.from_iterable(true_word_list))\nfrom collections import Counter\ntrue_mc = Counter(list1).most_common()\n \n \nfake = data[data['Label'] == -1]\nfake_word_list= fake['corpus'].str.split(expand=True).stack()\nfrom collections import Counter\nfake_mc = Counter(fake_word_list).most_common()\nfake_mc\n \ndf_true = pd.DataFrame(true_mc)\ndf_false = pd.DataFrame(fake_mc)\ndf_false1 = df_false[0:23]\ndf_true1 = df_true[0:23]\ndf_common = list(set(df_true1.iloc[:,0]) & set(df_false1.iloc[:,0]))\ndf_common","execution_count":4,"outputs":[{"output_type":"execute_result","execution_count":4,"data":{"text/plain":"['also',\n 'place',\n 'would',\n 'wait',\n 'food',\n 'order',\n 'love',\n 'great',\n 'restaur',\n 'get',\n 'servic',\n 'one',\n 'good',\n 'realli',\n 'tri',\n 'back',\n 'best',\n 'like',\n 'delici',\n 'go',\n 'come',\n 'time']"},"metadata":{}}]},{"metadata":{},"cell_type":"markdown","source":"Dropping the columns that are not required by the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"data = data.dropna()\nfinal_data_for_train =  data.drop(columns=['User_Id','Prod_Id','Date_x','Review ','Product_Name','compound'])","execution_count":5,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Vectorization and OneHotEncoding"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import RandomizedSearchCV\n \n#Split\nX_train, X_test, y_train, y_test = train_test_split(final_data_for_train, data['Label'], test_size=0.25, random_state=101)\n#Vectorize\nvectorizer = CountVectorizer(stop_words= df_common, ngram_range=(3,3), max_features= 15000)","execution_count":6,"outputs":[]},{"metadata":{},"cell_type":"raw","source":"Fitting and tranforming the corpus"},{"metadata":{"trusted":true},"cell_type":"code","source":"vectorizer.fit(X_train['corpus'])","execution_count":7,"outputs":[{"output_type":"execute_result","execution_count":7,"data":{"text/plain":"CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n        lowercase=True, max_df=1.0, max_features=15000, min_df=1,\n        ngram_range=(3, 3), preprocessor=None,\n        stop_words=['also', 'place', 'would', 'wait', 'food', 'order', 'love', 'great', 'restaur', 'get', 'servic', 'one', 'good', 'realli', 'tri', 'back', 'best', 'like', 'delici', 'go', 'come', 'time'],\n        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n        tokenizer=None, vocabulary=None)"},"metadata":{}}]},{"metadata":{"trusted":true},"cell_type":"code","source":"X_train_vector = vectorizer.transform(X_train['corpus'])\nX_test_vector  = vectorizer.transform(X_test['corpus'])","execution_count":8,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import scipy as sp\nfinal_train = sp.sparse.hstack((X_train_vector,X_train[['Rating','Avg_Prod_Rating','Avg_user_rating','Review_Len','user_total_reviews','neu','neg','pos','number_Cap_Words','number_digit_Words','noun_count']].values),format='csr')\nfinal_test = sp.sparse.hstack((X_test_vector,X_test[['Rating','Avg_Prod_Rating','Avg_user_rating','Review_Len','user_total_reviews','neu','neg','pos','number_Cap_Words','number_digit_Words','noun_count']].values),format='csr')","execution_count":9,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Random over sampling technique is used to handle unbalanced dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import RandomOverSampler\nros = RandomOverSampler(random_state=0)\nX_resampled, y_resampled = ros.fit_resample(final_train, y_train)\nfrom collections import Counter\nprint(sorted(Counter(y_resampled).items()))","execution_count":10,"outputs":[{"output_type":"stream","text":"Using TensorFlow backend.\n","name":"stderr"},{"output_type":"stream","text":"[(-1, 265849), (1, 265849)]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(X_resampled.shape)","execution_count":11,"outputs":[{"output_type":"stream","text":"(531698, 15011)\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Using Multinomial Naive bayes model as the data is discrete"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB \nmnb = MultinomialNB().fit(X_resampled,y_resampled)\n","execution_count":13,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mnb_pred=mnb.predict(final_test)","execution_count":14,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Metrics for evaluating the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, accuracy_score","execution_count":15,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(classification_report(mnb_pred, y_test))\nprint(roc_auc_score(mnb_pred, y_test))\nprint(confusion_matrix(mnb_pred, y_test))","execution_count":16,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n          -1       0.86      0.16      0.27     55389\n           1       0.48      0.97      0.64     43453\n\n   micro avg       0.52      0.52      0.52     98842\n   macro avg       0.67      0.56      0.46     98842\nweighted avg       0.69      0.52      0.43     98842\n\n0.5649108170681592\n[[ 8988 46401]\n [ 1410 42043]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(mnb_pred, y_test))","execution_count":17,"outputs":[{"output_type":"stream","text":"0.5162886222456041\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"SMOTE over sampling technique  used to balance the unbalanced dataset"},{"metadata":{"trusted":true},"cell_type":"code","source":"from imblearn.over_sampling import SMOTE\nsmote = SMOTE(random_state=0)\nX_resampled, y_resampled = smote.fit_resample(final_train, y_train)\nfrom collections import Counter\nprint(sorted(Counter(y_resampled).items()))","execution_count":18,"outputs":[{"output_type":"stream","text":"[(-1, 265849), (1, 265849)]\n","name":"stdout"}]},{"metadata":{},"cell_type":"markdown","source":"Using Multinomial Naive bayes model as the data is discrete"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.naive_bayes import MultinomialNB \nmnb = MultinomialNB().fit(X_resampled,y_resampled)\n","execution_count":19,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"mnb_pred=mnb.predict(final_test)","execution_count":20,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Metrics for evaluating the model"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix, accuracy_score","execution_count":21,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"x=classification_report(mnb_pred, y_test)\nprint(classification_report(mnb_pred, y_test))\nprint(roc_auc_score(mnb_pred, y_test))\nprint(confusion_matrix(mnb_pred, y_test))","execution_count":22,"outputs":[{"output_type":"stream","text":"              precision    recall  f1-score   support\n\n          -1       0.88      0.16      0.27     57574\n           1       0.45      0.97      0.62     41268\n\n   micro avg       0.50      0.50      0.50     98842\n   macro avg       0.67      0.56      0.44     98842\nweighted avg       0.70      0.50      0.41     98842\n\n0.5646334830234907\n[[ 9164 48410]\n [ 1234 40034]]\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(accuracy_score(mnb_pred, y_test))","execution_count":23,"outputs":[{"output_type":"stream","text":"0.49774387406163373\n","name":"stdout"}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}